{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03a12be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import random\n",
    "import uuid\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d36097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    KB_ID = os.getenv(\"KB_ID\")\n",
    "    MODEL_ARN = os.getenv(\"MODEL_ARN\")\n",
    "    DYNAMODB_TABLE_NAME = os.getenv(\"DYNAMODB_TABLE_NAME\")\n",
    "    AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "# Initialize AWS Bedrock Runtime Client\n",
    "client = boto3.client(\"bedrock-agent-runtime\", region_name=Config.AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af121a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    region_name=Config.AWS_REGION,\n",
    "    beta_use_converse_api=True,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0dc7e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def generate_modified_queries(question: str):\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "        different versions of the given user question to retrieve relevant documents from a \n",
    "        vector database. Provide these alternative questions separated by newlines. And only respond the questions along with the original question asked,\n",
    "        do not include any additional text or explanations.\n",
    "        Original question: {question}\"\"\"\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=QUERY_PROMPT)\n",
    "    result = chain.run({\"question\": question})\n",
    "    return result.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28517ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the benefits of our company's remote work policy?\n",
      "What advantages does our organization offer through its work-from-home policy?\n",
      "How does our company's flexible work arrangement benefit employees?\n",
      "What are the positive aspects of our firm's telecommuting policy?\n",
      "In what ways does our remote work program enhance employee experience?\n",
      "What perks do staff members enjoy from our company's virtual work setup?\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What are the benefits of our company's remote work policy?\"\n",
    "modified_queries = generate_modified_queries(user_question)\n",
    "\n",
    "for q in modified_queries:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88a4ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.dynamodb.conditions import Key,Attr\n",
    "dynamodb = boto3.resource('dynamodb',region_name=Config.AWS_REGION)\n",
    "TABLE_NAME = Config.DYNAMODB_TABLE_NAME\n",
    "table = dynamodb.Table(TABLE_NAME)\n",
    "\n",
    "class ChatHistoryManager:\n",
    "    def __init__(self, user_id: str = \"default\", session_id: str = \"default_session\"):\n",
    "        self.user_id = user_id\n",
    "        self.session_id = session_id\n",
    "\n",
    "    def load_history(self) -> List[Dict[str, Any]]:\n",
    "        today = datetime.now().date()\n",
    "        start_time = datetime.combine(today, datetime.min.time()).isoformat()\n",
    "        end_time = datetime.combine(today, datetime.max.time()).isoformat()\n",
    "\n",
    "        try:\n",
    "            response = table.query(\n",
    "                KeyConditionExpression=Key('user_id').eq(self.user_id) & Key('timestamp').between(start_time, end_time),\n",
    "                FilterExpression=Attr('session_id').eq(self.session_id)\n",
    "            )\n",
    "            return response.get('Items', [])\n",
    "        except Exception as e:\n",
    "            print(\"Error loading from DynamoDB:\", e)\n",
    "            return []\n",
    "\n",
    "    def save_history(self, history: List[Dict[str, Any]]):\n",
    "        for entry in history:\n",
    "            item = {\n",
    "                'user_id': self.user_id,\n",
    "                'timestamp': entry.get(\"timestamp\", datetime.now().isoformat()),  # sort key\n",
    "                'session_id': entry.get(\"session_id\", self.session_id),\n",
    "                'user_message': entry.get(\"user_message\", \"\"),\n",
    "                'assistant_response': entry.get(\"assistant_response\", \"\"),\n",
    "                'summarized': entry.get(\"summarized\", False)\n",
    "            }\n",
    "\n",
    "            if \"summary\" in entry:\n",
    "                item[\"summary\"] = entry[\"summary\"]\n",
    "            if \"summary_of\" in entry:\n",
    "                item[\"summary_of\"] = entry[\"summary_of\"]\n",
    "\n",
    "            try:\n",
    "                table.put_item(Item=item)\n",
    "            except Exception as e:\n",
    "                print(\"Error writing to DynamoDB:\", e)\n",
    "\n",
    "    def summarize_if_needed(self, history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        unsummarized_blocks = [\n",
    "            entry for entry in history\n",
    "            if not entry.get(\"summarized\", False)\n",
    "            and entry.get(\"user_message\") and entry.get(\"assistant_response\")\n",
    "        ]\n",
    "\n",
    "        if len(unsummarized_blocks) < 5:\n",
    "            return history\n",
    "\n",
    "        history_text = \"\\n\".join(\n",
    "            f\"User: {entry['user_message']}\\nAssistant: {entry['assistant_response']}\"\n",
    "            for entry in unsummarized_blocks[:10]\n",
    "        )\n",
    "\n",
    "        summary_prompt = f\"\"\"\n",
    "        Summarize the following 10 interactions into one concise but informative summary:\n",
    "        {history_text}\n",
    "        \"\"\"\n",
    "\n",
    "        summary = llm.invoke(summary_prompt.strip())\n",
    "        if isinstance(summary, AIMessage):\n",
    "            summary = summary.content\n",
    "\n",
    "        summary_entry = {\n",
    "            \"role\": \"system\",\n",
    "            \"summary\": summary,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"session_id\": self.session_id,\n",
    "            \"summarized\": True,\n",
    "            \"summary_of\": [entry[\"timestamp\"] for entry in unsummarized_blocks[:10]]\n",
    "        }\n",
    "\n",
    "        history = [entry for entry in history if entry not in unsummarized_blocks[:10]]\n",
    "        history.insert(0, summary_entry)\n",
    "        return history\n",
    "\n",
    "    def append_chat_pair(self, history: List[Dict[str, Any]], user_msg: str, assistant_msg: str) -> List[Dict[str, Any]]:\n",
    "        new_entry = {\n",
    "            \"user_message\": user_msg,\n",
    "            \"assistant_response\": assistant_msg,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"session_id\": self.session_id,\n",
    "            \"summarized\": False\n",
    "        }\n",
    "\n",
    "        history.append(new_entry)\n",
    "        self.save_history([new_entry])  # Save only the new one\n",
    "        return self.summarize_if_needed(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be4d3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_prompt(user_input: str, history: List) -> PromptTemplate:\n",
    "    sensitive_keywords = [\"complaint\", \"harassment\", \"grievance\", \"termination\"]\n",
    "    policy_keywords = [\"policy\", \"rule\", \"guideline\"]\n",
    "    benefit_keywords = [\"benefit\", \"pto\", \"leave\", \"insurance\"]\n",
    "    \n",
    "    if any(kw in user_input.lower() for kw in sensitive_keywords):\n",
    "        instructions = \"This is a sensitive topic. Be professional and direct the user to official HR channels if appropriate.\"\n",
    "    elif any(kw in user_input.lower() for kw in policy_keywords):\n",
    "        instructions = \"Provide exact policy details with reference to the policy document when possible.\"\n",
    "    elif any(kw in user_input.lower() for kw in benefit_keywords):\n",
    "        instructions = \"Include eligibility requirements and any limitations for benefits mentioned.\"\n",
    "    else:\n",
    "        instructions = \"Respond helpfully and professionally.\"\n",
    "    \n",
    "    template = f\"\"\"You are an HR assistant for a company. Use the following context to answer the question at the end.\n",
    "If you don't know the answer, say you don't know. Be concise but helpful.\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Conversation history:\n",
    "{{chat_history}}\n",
    "\n",
    "Question: {{input}}\n",
    "\n",
    "Considerations:\n",
    "1. {instructions}\n",
    "2. Format lists and important details clearly\n",
    "3. Provide sources when available\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "419b727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_kb(input):\n",
    "    try:\n",
    "        response = client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': Config.KB_ID,\n",
    "                    'modelArn': Config.MODEL_ARN\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving knowledge base: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de662ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_question_with_history(user_input: str, history: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Rewrites the user's question incorporating relevant context from the chat history.\n",
    "    \"\"\"\n",
    "    if not history:\n",
    "        return user_input\n",
    "    \n",
    "    # Convert history to conversational format\n",
    "    conversation_history = []\n",
    "    for entry in history[-5:]:  # Only use last 5 exchanges to avoid context overload\n",
    "        if \"user_message\" in entry:\n",
    "            conversation_history.append(HumanMessage(content=entry[\"user_message\"]))\n",
    "        if \"assistant_response\" in entry:\n",
    "            conversation_history.append(AIMessage(content=entry[\"assistant_response\"]))\n",
    "    \n",
    "    # Create a prompt to rewrite the question with context\n",
    "    rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"Given the conversation history, rephrase and expand this question to be more specific and clear: {input}. Only respond with the rewritten question.\"),\n",
    "    ])\n",
    "    \n",
    "    rewrite_chain = rewrite_prompt | llm\n",
    "    rewritten_question = rewrite_chain.invoke({\n",
    "        \"chat_history\": conversation_history,\n",
    "        \"input\": user_input\n",
    "    })\n",
    "    \n",
    "    if isinstance(rewritten_question, AIMessage):\n",
    "        return rewritten_question.content\n",
    "    return rewritten_question\n",
    "\n",
    "\n",
    "def chat(user_input: str, user_id: str = \"default\", session_id: str = \"default_session\") -> str:\n",
    "    history_manager = ChatHistoryManager(user_id, session_id)\n",
    "    chat_history = history_manager.load_history()\n",
    "    \n",
    "    # Step 1: Rewrite the question with history context\n",
    "    rewritten_question = rewrite_question_with_history(user_input, chat_history)\n",
    "    \n",
    "    # Step 2: Generate multiple query variations\n",
    "    modified_queries = generate_modified_queries(rewritten_question)\n",
    "    \n",
    "    # Step 3: Retrieve from knowledge base for each query variation\n",
    "    all_responses = []\n",
    "    for query in modified_queries:\n",
    "        kb_response = retrieve_from_kb(query)\n",
    "        if kb_response and 'output' in kb_response and kb_response['output']['text']:\n",
    "            all_responses.append(kb_response['output']['text'])\n",
    "    \n",
    "    # Combine all retrieved context\n",
    "    combined_context = \"\\n\\n\".join(all_responses) if all_responses else \"No relevant context found.\"\n",
    "    \n",
    "    # Step 4: Generate final response with dynamic prompt\n",
    "    prompt_template = get_dynamic_prompt(user_input, chat_history)\n",
    "    prompt = prompt_template.format(\n",
    "        context=combined_context,\n",
    "        chat_history=\"\\n\".join([\n",
    "            f\"User: {entry['user_message']}\\nAssistant: {entry['assistant_response']}\" \n",
    "            for entry in chat_history[-5:]  # Only show last 5 exchanges\n",
    "            if 'user_message' in entry and 'assistant_response' in entry\n",
    "        ]),\n",
    "        input=user_input\n",
    "    )\n",
    "    \n",
    "    # Get final response from LLM\n",
    "    final_response = llm.invoke(prompt)\n",
    "    if isinstance(final_response, AIMessage):\n",
    "        final_response = final_response.content\n",
    "    \n",
    "    # Step 5: Save the interaction to history\n",
    "    history_manager.append_chat_pair(chat_history, user_input, final_response)\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced58e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "response1 = chat(\"What is our remote work policy?\", \"user123\", \"session456\")\n",
    "print(response1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"what do you know about the CEO of AyataCommerce?\",\"123\",\"1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3647385",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"what do you know about him?\",\"123\",\"1234\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
