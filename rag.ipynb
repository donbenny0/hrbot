{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "urhaZfsxkjKW"
      },
      "outputs": [],
      "source": [
        " pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jUm3RKmBkJ95"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y-00yGx0lhzv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DonBenny\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "embedding_model  = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BMYH05r_lx1U"
      },
      "outputs": [],
      "source": [
        "file_path = \"docs/policies.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "03TN8Mw-mE2n"
      },
      "outputs": [],
      "source": [
        "def read_docs():\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u3tcmFnHmVHu"
      },
      "outputs": [],
      "source": [
        "def create_initial_chunks():\n",
        "    text = read_docs()\n",
        "    # Split text by periods but keep the periods with the preceding text\n",
        "    paragraphs = [p + '.' for p in re.split(r'\\.', text)[:-1]]\n",
        "    # Add the last chunk without adding an extra period\n",
        "    if text and not text.endswith('.'):\n",
        "        paragraphs.append(re.split(r'\\.', text)[-1])\n",
        "    print(f\"Total paragraphs: {len(paragraphs)}\\n\")\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j2N3TEwonaCW"
      },
      "outputs": [],
      "source": [
        "def create_semantic_chunks():\n",
        "    paragraphs = create_initial_chunks()\n",
        "    # Fixed this line to embed each paragraph individually\n",
        "    para_embeddings = [np.array(embedding_model.embed_query(paragraph)).reshape(1,-1) for paragraph in paragraphs]\n",
        "\n",
        "    semantic_chunks = []\n",
        "    for i in range(len(paragraphs)):\n",
        "        if i == 0:\n",
        "            semantic_chunks.append([paragraphs[i]])\n",
        "        else:\n",
        "            similarity = cosine_similarity(para_embeddings[i-1], para_embeddings[i])\n",
        "            if similarity[0][0] > 0.5:\n",
        "                semantic_chunks[-1].append(paragraphs[i])\n",
        "            else:\n",
        "                semantic_chunks.append([paragraphs[i]])\n",
        "\n",
        "    return semantic_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puecvdAiA2CR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DonBenny\\AppData\\Local\\Temp\\ipykernel_4604\\2979283942.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n"
          ]
        }
      ],
      "source": [
        "persist_directory = \"chroma_store\"\n",
        "collection_name = \"semantic_chunks\"\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    collection_name=collection_name,\n",
        "    embedding_function=embedding_model,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_chunks_in_chroma():\n",
        "    semantic_chunks = create_semantic_chunks()\n",
        "    docs = []\n",
        "\n",
        "    for idx, chunk_group in enumerate(semantic_chunks):\n",
        "        combined_text = ' '.join(chunk_group).strip()\n",
        "        doc = Document(page_content=combined_text, metadata={\"chunk_id\": idx})\n",
        "        docs.append(doc)\n",
        "\n",
        "    vectorstore.add_documents(docs)\n",
        "    vectorstore.persist()\n",
        "    print(f\"Stored {len(docs)} semantic chunks in Chroma.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_chunks(question: str, top_k: int = 3):\n",
        "    print(f\"\\nQuery: {question}\")\n",
        "    results = vectorstore.similarity_search(question, k=top_k)\n",
        "    for i, doc in enumerate(results):\n",
        "        print(f\"\\n--- Result {i+1} ---\")\n",
        "        print(doc.page_content)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_ollama(question: str):\n",
        "    \"\"\"Send query to local Ollama instance with improved prompt\"\"\"\n",
        "    results = query_chunks(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in results])\n",
        "    try:\n",
        "        prompt = f\"\"\"\"Use the following context to answer the question.\n",
        "            \n",
        "        Rules:\n",
        "        1. If the context contains relevant information, provide a concise answer based solely on that.\n",
        "        2. If the question asks about something NOT in the context, respond ONLY with: 'I don't have this information in my knowledge base. Please contact hr@ayatacommerce.com for assistance.'\n",
        "        3. Never infer or make up information not explicitly stated in the context.\n",
        "        4. If the question is ambiguous or unclear, ask for clarification.\n",
        "            \n",
        "        Context: {context}\n",
        "            \n",
        "        Question: {question}\n",
        "            \n",
        "        Answer:\"\"\"\n",
        "            \n",
        "        response = requests.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": \"llama3.2:3b\",\n",
        "                \"prompt\": prompt,\n",
        "                \"stream\": False,\n",
        "                \"options\": {\n",
        "                    \"temperature\": 0.7,\n",
        "                    \"top_k\": 50,\n",
        "                    \"top_p\": 0.9\n",
        "                }\n",
        "            },\n",
        "            timeout=60\n",
        "        )\n",
        "            \n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            answer = response_data.get('response', 'No response generated')\n",
        "                \n",
        "            # Post-process answer to ensure compliance with rules\n",
        "            if \"I don't know\" in answer or \"I'm not sure\" in answer:\n",
        "                return \"I don't have this information in my knowledge base. Please contact hr@greenways.com for assistance.\"\n",
        "            return answer\n",
        "        else:\n",
        "            return f\"Error: {response.status_code} - {response.text}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error querying Ollama: {str(e)}\"\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total paragraphs: 226\n",
            "\n",
            "Stored 56 semantic chunks in Chroma.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\DonBenny\\AppData\\Local\\Temp\\ipykernel_4604\\3944027839.py:11: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "store_chunks_in_chroma()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: Is it possible to apply for Sick Leave and Earned Leave on consecutive days?\n",
            "\n",
            "--- Result 1 ---\n",
            "&procedures\n",
            "Working Arrangements\n",
            "HOURS\n",
            "Unless specified otherwise in your contract you are required to work 8hrs per day.  We trust you to determine what \n",
            "works for you with regards to when your working day begins and ends.  \n",
            "ATTENDANCE\n",
            "If you are unable to work because of sickness or an emergency, you must ensure your manager is informed as early \n",
            "as possible on the first day of absence.  \n",
            "HOLIDAYS\n",
            "If you work full-time on the India payroll you are entitled to 12 days of Earned Leaves each holiday year, 12 days of \n",
            "Casual/Sick Leaves.  The holiday year runs from 1 January to 31 December.  The timing of all holidays should be \n",
            "agreed upon with your manager as early as possible (for details please refer to the detailed Leave Guidelines).  We \n",
            "would prefer staff to take their full holiday entitlement in the leave year to which it relates.  However, it is \n",
            "recognized that unused holidays may be left at the end of the year.  Therefore, up to 10 days of Earned leaves can \n",
            "be carried over to the following year to a maximum of 30 leave balances. \n",
            "PUBLIC HOLIDAYS \n",
            "In addition to the annual holiday allowance, you are entitled to a paid holiday on the 12 public and bank holidays \n",
            "(inclusive of the flexible leaves).\n",
            "\n",
            "--- Result 2 ---\n",
            "Health and Wellbeing\n",
            "ABSENCE FROM WORK LONG-TERM\n",
            "A period of absence of 4 continuous weeks or longer might be deemed to be ‘long term absence’.  In the event of \n",
            "long-term absence we may consider altering your job functions or conditions or adjusting your working \n",
            "arrangements to accommodate your needs in accordance with the terms of the Equality Act 2010. \n",
            "SICKNESS WHILST ON ANNUAL LEAVE\n",
            "If you fall sick whilst on holiday you must notify your manager as soon as possible.  To be eligible for this time to \n",
            "be treated as absence owing to sickness rather than annual leave, and to be eligible for CSP in respect of this \n",
            "time, you must produce a medical certificate to confirm your sickness as soon as is reasonably practical.  The \n",
            "decision as to whether to treat periods of sickness whilst on annual leave as sickness absence is at the company’s \n",
            "discretion.  In case you exhaust your Sick/Casual leaves, the leaves during the sickness shall be adjusted against \n",
            "the earned leaves. \n",
            "RETURNING TO WORK\n",
            "After a period of absence, you may be required to attend a return to work meeting with your manager.  This will \n",
            "help establish whether any steps or additional support can be given. \n",
            "Health and Wellbeing\n",
            "Identify your triggers \n",
            "We all have times where we \n",
            "feel stressed, upset or find it \n",
            "difficult to cope.  Working \n",
            "out what triggers poor \n",
            "mental health for you can \n",
            "help you anticipate \n",
            "problems and find solutions.  \n",
            "Whether it’s taking in too \n",
            "much negative news and \n",
            "media regarding the current \n",
            "circumstance or a heavy \n",
            "workload.  Finding out your \n",
            "triggers can help.\n",
            "\n",
            "--- Result 3 ---\n",
            "Health and Wellbeing\n",
            "ABSENCE FROM WORK SHORT-TERM\n",
            "Working when unwell can be detrimental to your health both physically and mentally.  It could prolong any illness \n",
            "or injury and could compromise your cognitive abilities and decision-making, leading to poor judgement, work \n",
            "errors and under-performance, which is why we strongly encourage you to rest up and recuperate.  \n",
            "We trust you to be responsible and exercise judgement: It is therefore your responsibility and prerogative to \n",
            "determine if you are well enough to continue working. \n",
            "ACTIONS\n",
            "• Ensure your manager knows as soon as possible on the first day of absence.  \n",
            "• Keep us informed about your sickness and its expected duration.  If you are going to be away from work for \n",
            "more than 48 hours, you must inform your manager. \n",
            "• Make sure you understand the rules about company sick pay.  Ask payroll to clarify any matter which is unclear \n",
            "to you.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I don't have this information in my knowledge base. Please contact hr@greenways.com for assistance.\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Then you can run: ===\n",
        "# query_ollama(\"What is a full day work at AyataCommerce in terms of hours?\")\n",
        "query_ollama(\"Is it possible to apply for Sick Leave and Earned Leave on consecutive days?\")  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
