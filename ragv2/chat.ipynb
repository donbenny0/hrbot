{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c673621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement rank_bm25a (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for rank_bm25a\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9307eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6c7261ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    API_VERSION = \"2024-02-01\"\n",
    "    PERSIST_DIRECTORY = \"askhr_bot_vectorstore\"\n",
    "    COLLECTION_NAME = \"askhr_bot_vectorstore_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0db1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "    openai_api_version=Config.API_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1cfd29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=Config.API_VERSION,\n",
    "    deployment_name=Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ffc3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=Config.COLLECTION_NAME,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=Config.PERSIST_DIRECTORY,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "90b96262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_retrievers():\n",
    "    try:\n",
    "        raw = vectorstore.get(include=[\"documents\", \"metadatas\"])\n",
    "        docs = [\n",
    "            Document(page_content=content, metadata=metadata)\n",
    "            for content, metadata in zip(raw[\"documents\"], raw[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs, k=5)\n",
    "\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 7, \"fetch_k\": 20, \"lambda_mult\": 0.6, \"score_threshold\": 0.7}\n",
    "        )\n",
    "\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[bm25_retriever, vector_retriever],\n",
    "            weights=[0.4, 0.6]\n",
    "        )\n",
    "\n",
    "        QUERY_PROMPT = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "            different versions of the given user question to retrieve relevant documents from a \n",
    "            vector database. Provide these alternative questions separated by newlines.\n",
    "            Original question: {question}\"\"\"\n",
    "        )\n",
    "\n",
    "        return MultiQueryRetriever.from_llm(\n",
    "            retriever=ensemble_retriever,\n",
    "            llm=llm,\n",
    "            prompt=QUERY_PROMPT,\n",
    "            include_original=True\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing retrievers: {e}. Falling back to simple retriever.\")\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d9f10fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = initialize_retrievers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e9f095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHistoryManager:\n",
    "    def __init__(self, user_id: str = \"default\"):\n",
    "        self.user_id = user_id\n",
    "        self.history_file = f\"chat_history_{user_id}.json\"\n",
    "        \n",
    "    def load_history(self) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            return []\n",
    "        \n",
    "    def save_history(self, history: List[Dict[str, Any]]):\n",
    "        with open(self.history_file, 'w') as f:\n",
    "            json.dump(history[-20:], f)\n",
    "    \n",
    "    def summarize_history(self, history: List[Dict[str, Any]]):\n",
    "        if len(history) < 8:\n",
    "            return history\n",
    "        \n",
    "        summary_prompt = \"\"\"\n",
    "        Summarize the key points from this conversation while preserving important details:\n",
    "        {history}\n",
    "        \"\"\"\n",
    "        history_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in history])\n",
    "        summary = llm.invoke(summary_prompt.format(history=history_text))\n",
    "        \n",
    "        return [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Conversation summary: {summary}\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }] + history[-10:]\n",
    "    \n",
    "    def manage_history(self, history: List[Dict[str, Any]], new_msg: Dict[str, Any]):\n",
    "        history.append({\n",
    "            **new_msg,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        if len(history) > 15:\n",
    "            history = self.summarize_history(history[:10]) + history[10:]\n",
    "        return history[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58e20417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseEvaluator:\n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "    \n",
    "    def log_interaction(self, user_input, response, context, retrieval_time):\n",
    "        self.evaluation_history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"input\": user_input,\n",
    "            \"response\": response,\n",
    "            \"context_relevance\": self._calculate_context_relevance(response, context),\n",
    "            \"retrieval_time\": retrieval_time\n",
    "        })\n",
    "        self.evaluation_history = self.evaluation_history[-100:]\n",
    "        \n",
    "    def _calculate_context_relevance(self, response, context):\n",
    "        if not context:\n",
    "            return 0\n",
    "        context_keywords = set(\" \".join(context).split())\n",
    "        response_keywords = set(response.split())\n",
    "        common = context_keywords & response_keywords\n",
    "        return len(common) / len(context_keywords) if context_keywords else 0\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.evaluation_history:\n",
    "            return {}\n",
    "        avg_relevance = sum(\n",
    "            e[\"context_relevance\"] for e in self.evaluation_history\n",
    "        ) / len(self.evaluation_history)\n",
    "        avg_time = sum(\n",
    "            e[\"retrieval_time\"] for e in self.evaluation_history\n",
    "        ) / len(self.evaluation_history)\n",
    "        return {\n",
    "            \"avg_context_relevance\": avg_relevance,\n",
    "            \"avg_response_time\": avg_time,\n",
    "            \"total_interactions\": len(self.evaluation_history)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "366f5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ResponseEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e92262df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_prompt(user_input: str, history: List) -> PromptTemplate:\n",
    "    sensitive_keywords = [\"complaint\", \"harassment\", \"grievance\", \"termination\"]\n",
    "    policy_keywords = [\"policy\", \"rule\", \"guideline\"]\n",
    "    benefit_keywords = [\"benefit\", \"pto\", \"leave\", \"insurance\"]\n",
    "    \n",
    "    if any(kw in user_input.lower() for kw in sensitive_keywords):\n",
    "        instructions = \"This is a sensitive topic. Be professional and direct the user to official HR channels if appropriate.\"\n",
    "    elif any(kw in user_input.lower() for kw in policy_keywords):\n",
    "        instructions = \"Provide exact policy details with reference to the policy document when possible.\"\n",
    "    elif any(kw in user_input.lower() for kw in benefit_keywords):\n",
    "        instructions = \"Include eligibility requirements and any limitations for benefits mentioned.\"\n",
    "    else:\n",
    "        instructions = \"Respond helpfully and professionally.\"\n",
    "    \n",
    "    template = f\"\"\"You are an HR assistant for a company. Use the following context to answer the question at the end.\n",
    "If you don't know the answer, say you don't know. Be concise but helpful.\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Conversation history:\n",
    "{{chat_history}}\n",
    "\n",
    "Question: {{input}}\n",
    "\n",
    "Considerations:\n",
    "1. {instructions}\n",
    "2. Format lists and important details clearly\n",
    "3. Provide sources when available\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a8615ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_serializable(docs: List[Document]) -> List[Dict[str, Any]]:\n",
    "    return [\n",
    "        {\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cab651c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str, user_id: str = \"default\") -> str:\n",
    "    history_manager = ChatHistoryManager(user_id)\n",
    "    chat_history = history_manager.load_history()\n",
    "    \n",
    "    chat_history = history_manager.manage_history(chat_history, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input\n",
    "    })\n",
    "    \n",
    "    contextualize_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given a chat history and the latest user question, formulate a standalone question. \"\n",
    "                  \"Do NOT answer the question, just reformulate it if needed.\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_prompt\n",
    "    )\n",
    "    \n",
    "    qa_prompt = get_dynamic_prompt(user_input, chat_history)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    \n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    start_time = time()\n",
    "    try:\n",
    "        response = rag_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": [\n",
    "                (msg[\"role\"], msg[\"content\"]) \n",
    "                for msg in chat_history \n",
    "                if msg[\"role\"] in (\"user\", \"assistant\")\n",
    "            ]\n",
    "        })\n",
    "        elapsed = time() - start_time\n",
    "        \n",
    "        context_serialized = docs_to_serializable(response.get(\"context\", []))\n",
    "        \n",
    "        chat_history = history_manager.manage_history(chat_history, {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response[\"answer\"],\n",
    "            \"sources\": context_serialized\n",
    "        })\n",
    "        \n",
    "        history_manager.save_history(chat_history)\n",
    "        evaluator.log_interaction(\n",
    "            user_input=user_input,\n",
    "            response=response[\"answer\"],\n",
    "            context=[doc[\"content\"] for doc in context_serialized],\n",
    "            retrieval_time=elapsed\n",
    "        )\n",
    "        \n",
    "        if len(evaluator.evaluation_history) % 10 == 0:\n",
    "            print(f\"Performance Metrics: {evaluator.get_metrics()}\")\n",
    "        \n",
    "        return response[\"answer\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG chain: {e}\")\n",
    "        fallback_responses = [\n",
    "            f\"I'm having trouble accessing that information. Could you rephrase your question? (Error: {str(e)[:50]})\",\n",
    "            \"My knowledge base seems to be unavailable at the moment. Please try again later.\",\n",
    "            \"I encountered an unexpected error while processing your request.\"\n",
    "        ]\n",
    "        return random.choice(fallback_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4313e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have specific information about the CEO of AyataCommerce beyond what is mentioned in the context provided. The CEO is Shine Mathew, who is also the founder of AyataCommerce. He welcomes employees to the company and emphasizes the importance of the organizational culture and values. For more detailed information about his background or achievements, you may need to refer to the company's official website or press releases.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"what do you know about the CEO of AyataCommerce?\",\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c50d19b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have any information about a specific individual. If you have a particular person in mind, please provide their name or context, and I can assist you with general information or guidance related to HR policies or procedures.\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"what do you know about him?\",\"1234\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
