{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c673621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement rank_bm25a (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for rank_bm25a\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7261ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    API_VERSION = \"2024-02-01\"\n",
    "    PERSIST_DIRECTORY = \"askhr_bot_vectorstore\"\n",
    "    COLLECTION_NAME = \"askhr_bot_vectorstore_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0db1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "    openai_api_version=Config.API_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1cfd29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=Config.API_VERSION,\n",
    "    deployment_name=Config.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24696368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatWithLLM(prompt: str) -> str:\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        if isinstance(response, AIMessage):\n",
    "            return response.content\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM invocation: {e}\")\n",
    "        return \"I'm having trouble processing that request right now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca5a2299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sree Narayana Guru (1856–1928) was a prominent social reformer, spiritual leader, and philosopher from Kerala, India. He is best known for his efforts to promote social equality and uplift the marginalized communities in the caste-ridden society of his time. Guru emphasized the importance of education, self-respect, and social justice.\\n\\nSree Narayana Guru founded the \"Sree Narayana Dharma Paripalana (SNDP) Yogam\" in 1903, which aimed to improve the social and economic conditions of the backward classes in Kerala. He advocated for the idea of \"One Caste, One Religion, One God for All,\" promoting unity and harmony among different communities.\\n\\nHe also established various temples and educational institutions, emphasizing the need for education as a means of empowerment. His teachings and philosophy continue to inspire many in Kerala and beyond, and he is revered as a saint and a key figure in the fight against social injustice and inequality. His legacy is celebrated through various events and organizations dedicated to his ideals.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatWithLLM(\"Who is Sreen narayana guru\")  # Test LLM invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fa920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ffc3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=Config.COLLECTION_NAME,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=Config.PERSIST_DIRECTORY,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b96262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_retrievers():\n",
    "    try:\n",
    "        raw = vectorstore.get(include=[\"documents\", \"metadatas\"])\n",
    "        docs = [\n",
    "            Document(page_content=content, metadata=metadata)\n",
    "            for content, metadata in zip(raw[\"documents\"], raw[\"metadatas\"])\n",
    "        ]\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs, k=5)\n",
    "\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 7, \"fetch_k\": 20, \"lambda_mult\": 0.6, \"score_threshold\": 0.7}\n",
    "        )\n",
    "\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[bm25_retriever, vector_retriever],\n",
    "            weights=[0.4, 0.6]\n",
    "        )\n",
    "\n",
    "        QUERY_PROMPT = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "            different versions of the given user question to retrieve relevant documents from a \n",
    "            vector database. Provide these alternative questions separated by newlines.\n",
    "            Original question: {question}\"\"\"\n",
    "        )\n",
    "\n",
    "        return MultiQueryRetriever.from_llm(\n",
    "            retriever=ensemble_retriever,\n",
    "            llm=llm,\n",
    "            prompt=QUERY_PROMPT,\n",
    "            include_original=True\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing retrievers: {e}. Falling back to simple retriever.\")\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f10fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = initialize_retrievers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e9f095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatHistoryManager:\n",
    "    def __init__(self, user_id: str = \"default\", session_id: str = \"default_session\"):\n",
    "        self.user_id = user_id\n",
    "        self.session_id = session_id\n",
    "        self.history_file = f\"chat_history_{user_id}.json\"\n",
    "\n",
    "    def load_history(self) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as f:\n",
    "                all_history = json.load(f)\n",
    "            return [entry for entry in all_history if entry.get(\"session_id\") == self.session_id]\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            return []\n",
    "\n",
    "    def save_history(self, history: List[Dict[str, Any]]):\n",
    "        try:\n",
    "            with open(self.history_file, 'r') as f:\n",
    "                all_history = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            all_history = []\n",
    "\n",
    "        # remove old session data\n",
    "        all_history = [entry for entry in all_history if entry.get(\"session_id\") != self.session_id]\n",
    "        all_history.extend(history)\n",
    "        with open(self.history_file, 'w') as f:\n",
    "            json.dump(all_history[-100:], f, indent=2)\n",
    "\n",
    "    def summarize_if_needed(self, history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        unsummarized_blocks = [\n",
    "            entry for entry in history\n",
    "            if not entry.get(\"summarized\", False)\n",
    "            and entry.get(\"user_message\") and entry.get(\"assistant_response\")\n",
    "        ]\n",
    "\n",
    "        if len(unsummarized_blocks) < 5:\n",
    "            return history\n",
    "\n",
    "        history_text = \"\\n\".join(\n",
    "            f\"User: {entry['user_message']}\\nAssistant: {entry['assistant_response']}\"\n",
    "            for entry in unsummarized_blocks[:10]\n",
    "        )\n",
    "\n",
    "        summary_prompt = f\"\"\"\n",
    "        Summarize the following 10 interactions into one concise but informative summary:\n",
    "        {history_text}\n",
    "        \"\"\"\n",
    "\n",
    "        summary = llm.invoke(summary_prompt.strip())\n",
    "\n",
    "        # ✅ Fix: Ensure AIMessage is converted to string\n",
    "        if isinstance(summary, AIMessage):\n",
    "            summary = summary.content\n",
    "\n",
    "        summary_entry = {\n",
    "            \"role\": \"system\",\n",
    "            \"summary\": summary,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"session_id\": self.session_id,\n",
    "            \"summarized\": True,\n",
    "            \"summary_of\": [entry[\"timestamp\"] for entry in unsummarized_blocks[:10]]\n",
    "        }\n",
    "\n",
    "        # Replace the first 10 unsummarized entries with the summary\n",
    "        history = [entry for entry in history if entry not in unsummarized_blocks[:10]]\n",
    "        history.insert(0, summary_entry)\n",
    "        return history\n",
    "\n",
    "    def append_chat_pair(self, history: List[Dict[str, Any]], user_msg: str, assistant_msg: str) -> List[Dict[str, Any]]:\n",
    "        history.append({\n",
    "            \"user_message\": user_msg,\n",
    "            \"assistant_response\": assistant_msg,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"session_id\": self.session_id,\n",
    "            \"summarized\": False\n",
    "        })\n",
    "        return self.summarize_if_needed(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e20417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseEvaluator:\n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "    \n",
    "    def log_interaction(self, user_input, response, context, retrieval_time):\n",
    "        self.evaluation_history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"input\": user_input,\n",
    "            \"response\": response,\n",
    "            \"context_relevance\": self._calculate_context_relevance(response, context),\n",
    "            \"retrieval_time\": retrieval_time\n",
    "        })\n",
    "        self.evaluation_history = self.evaluation_history[-100:]\n",
    "        \n",
    "    def _calculate_context_relevance(self, response, context):\n",
    "        if not context:\n",
    "            return 0\n",
    "        context_keywords = set(\" \".join(context).split())\n",
    "        response_keywords = set(response.split())\n",
    "        common = context_keywords & response_keywords\n",
    "        return len(common) / len(context_keywords) if context_keywords else 0\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.evaluation_history:\n",
    "            return {}\n",
    "        avg_relevance = sum(\n",
    "            e[\"context_relevance\"] for e in self.evaluation_history\n",
    "        ) / len(self.evaluation_history)\n",
    "        avg_time = sum(\n",
    "            e[\"retrieval_time\"] for e in self.evaluation_history\n",
    "        ) / len(self.evaluation_history)\n",
    "        return {\n",
    "            \"avg_context_relevance\": avg_relevance,\n",
    "            \"avg_response_time\": avg_time,\n",
    "            \"total_interactions\": len(self.evaluation_history)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366f5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ResponseEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92262df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_prompt(user_input: str, history: List) -> PromptTemplate:\n",
    "    sensitive_keywords = [\"complaint\", \"harassment\", \"grievance\", \"termination\"]\n",
    "    policy_keywords = [\"policy\", \"rule\", \"guideline\"]\n",
    "    benefit_keywords = [\"benefit\", \"pto\", \"leave\", \"insurance\"]\n",
    "    \n",
    "    if any(kw in user_input.lower() for kw in sensitive_keywords):\n",
    "        instructions = \"This is a sensitive topic. Be professional and direct the user to official HR channels if appropriate.\"\n",
    "    elif any(kw in user_input.lower() for kw in policy_keywords):\n",
    "        instructions = \"Provide exact policy details with reference to the policy document when possible.\"\n",
    "    elif any(kw in user_input.lower() for kw in benefit_keywords):\n",
    "        instructions = \"Include eligibility requirements and any limitations for benefits mentioned.\"\n",
    "    else:\n",
    "        instructions = \"Respond helpfully and professionally.\"\n",
    "    \n",
    "    template = f\"\"\"You are an HR assistant for a company. Use the following context to answer the question at the end.\n",
    "If you don't know the answer, say you don't know. Be concise but helpful.\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Conversation history:\n",
    "{{chat_history}}\n",
    "\n",
    "Question: {{input}}\n",
    "\n",
    "Considerations:\n",
    "1. {instructions}\n",
    "2. Format lists and important details clearly\n",
    "3. Provide sources when available\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8615ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_serializable(docs: List[Document]) -> List[Dict[str, Any]]:\n",
    "    return [\n",
    "        {\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab651c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str, user_id: str = \"default\", session_id: str = \"default_session\") -> str:\n",
    "    history_manager = ChatHistoryManager(user_id, session_id)\n",
    "    chat_history = history_manager.load_history()\n",
    "\n",
    "    formatted_history = []\n",
    "    for entry in chat_history:\n",
    "        if entry.get(\"summarized\", False):\n",
    "            formatted_history.append((\"system\", entry[\"summary\"]))\n",
    "        else:\n",
    "            formatted_history.append((\"user\", entry[\"user_message\"]))\n",
    "            formatted_history.append((\"assistant\", entry[\"assistant_response\"]))\n",
    "\n",
    "\n",
    "\n",
    "    contextualize_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given a chat history and the latest user question, formulate a standalone question.\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_prompt)\n",
    "    qa_prompt = get_dynamic_prompt(user_input, chat_history)\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "    start_time = time()\n",
    "    try:\n",
    "        response = rag_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": formatted_history\n",
    "        })\n",
    "        elapsed = time() - start_time\n",
    "\n",
    "        # Make sure response[\"answer\"] is a plain string\n",
    "        answer = response[\"answer\"]\n",
    "        if isinstance(answer, AIMessage):\n",
    "            answer = answer.content\n",
    "\n",
    "\n",
    "        chat_history = history_manager.append_chat_pair(\n",
    "            chat_history, user_msg=user_input, assistant_msg=answer\n",
    "        )\n",
    "\n",
    "        history_manager.save_history(chat_history)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG chain: {e}\")\n",
    "        fallback_responses = [\n",
    "            f\"I'm having trouble accessing that information. Could you rephrase your question? (Error: {str(e)[:50]})\",\n",
    "            \"My knowledge base seems to be unavailable at the moment. Please try again later.\",\n",
    "            \"I encountered an unexpected error while processing your request.\"\n",
    "        ]\n",
    "        return random.choice(fallback_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4313e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The CEO of AyataCommerce is Shine Mathew. Here are some key details about him:\\n\\n- **Role**: Founder and CEO of AyataCommerce.\\n- **Vision**: He emphasizes the importance of a positive work culture and values such as empathy, trust, and adaptability within the organization.\\n- **Company Journey**: Under his leadership, AyataCommerce has expanded its operations and established partnerships, contributing to its growth in the e-commerce sector.\\n\\nFor more specific information about his background or achievements, I don't have additional details available.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"what do you know about the CEO of AyataCommerce?\",\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c50d19b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shine Mathew is the CEO and Founder of AyataCommerce. Here are some key details about him:\\n\\n- **Role**: Founder and CEO of AyataCommerce.\\n- **Vision**: He emphasizes the importance of a positive work culture and values such as empathy, trust, and adaptability within the organization.\\n- **Company Journey**:\\n  - **2016**: AyataCommerce was founded.\\n  - **2017**: Established a partnership with SAP.\\n  - **2019**: Launched a new website.\\n  - **2021**: Opened an office in Kochi.\\n  - **2022**: Expanded operations to include a regional office in Bangalore.\\n\\nFor more specific information about his background or achievements, I don't have additional details available.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"what do you know about him?\",\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7520333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The core values of AyataCommerce are:\\n\\n1. **Empathy**: Understanding and valuing diverse perspectives and behaviors in interactions with clients and colleagues.\\n2. **Trust**: Building a culture of mutual trust that encourages employees to make decisions and take well-thought-out risks.\\n3. **Adaptability**: Embracing flexibility and innovation to address uncertainties and adjust to new conditions effectively.\\n\\nThese values guide the company's working practices and interactions both internally and externally.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"What are the core values of the company?\",\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12991199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in RAG chain: Object of type AIMessage is not JSON serializable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My knowledge base seems to be unavailable at the moment. Please try again later.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Which among the values reflects about the flexibility?\",\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7088a6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, it is generally acceptable to take a half-day leave for sudden illness at AyataCommerce, provided you notify your manager promptly. Here are the key points to consider:\\n\\n### Eligibility Requirements:\\n- **Notification**: Inform your manager as soon as possible on the first day of absence.\\n- **Documentation**: If your absence extends beyond 2 days, you will need a medical certificate from your General Physician.\\n\\n### Limitations:\\n- **Sick Leave Balance**: Ensure you have sufficient sick leave available. If you exhaust your Sick/Casual leaves, the leave taken during sickness may be adjusted against your earned leaves.\\n- **Approval**: The timing of your leave should be agreed upon with your manager as early as possible.\\n\\nFor more details, refer to the company's Leave Guidelines or contact your designated HR representative.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Is it ok to take a half day leave if i am sick suddenly?\",\"123\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
