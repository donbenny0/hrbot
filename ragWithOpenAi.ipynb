{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain-huggingface chromadb langchain-community langchain-core sentence-transformers groq langchain_groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "855395d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "azure_openapi_azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openapi_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openapi_deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "azure_openapi_api_version=\"2024-12-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5c7277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=azure_openapi_azure_endpoint,\n",
    "    api_key=azure_openapi_api_key,\n",
    "    openai_api_version=azure_openapi_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03f3cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[r\"\\n\\n\", r\"\\n\", r\"\\. \", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4cecef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b19db76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_chunks(file_path: str) -> List[str]:\n",
    "    \n",
    "    text = read_docs(file_path)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    documents = text_splitter.create_documents([text])\n",
    "    return [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52e5db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunks(paragraphs: List[str], \n",
    "                         similarity_threshold: float = 0.82) -> List[List[str]]:\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "    \n",
    "    # Batch process all embeddings at once\n",
    "    para_embeddings = embedding_model.embed_documents(paragraphs)\n",
    "    para_embeddings = [np.array(e).reshape(1, -1) for e in para_embeddings]\n",
    "    \n",
    "    semantic_chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in range(len(paragraphs)):\n",
    "        if not current_chunk:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "            continue\n",
    "            \n",
    "        # Compare with all paragraphs in current chunk\n",
    "        similarities = [cosine_similarity(para_embeddings[i], e)[0][0] \n",
    "                       for e in para_embeddings[:i]]\n",
    "        max_similarity = max(similarities) if similarities else 0\n",
    "        \n",
    "        if max_similarity > similarity_threshold:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "        else:\n",
    "            semantic_chunks.append(current_chunk)\n",
    "            current_chunk = [paragraphs[i]]\n",
    "    \n",
    "    if current_chunk:\n",
    "        semantic_chunks.append(current_chunk)\n",
    "        \n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "246bd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure vector store with optimized settings\n",
    "persist_directory = \"vectorstore_persist_optimized\"\n",
    "collection_name = \"vectorstore_table_optimized\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71267d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_in_chroma(semantic_chunks: List[List[str]]) -> str:\n",
    "    \"\"\"Store semantic chunks in Chroma with optimized metadata.\"\"\"\n",
    "    if not semantic_chunks:\n",
    "        return \"No chunks to store.\"\n",
    "    \n",
    "    docs = []\n",
    "    for idx, chunk_group in enumerate(semantic_chunks):\n",
    "        combined_text = ' '.join(chunk_group).strip()\n",
    "        if not combined_text:\n",
    "            continue\n",
    "            \n",
    "        # Extract first few words as title for better metadata\n",
    "        title = ' '.join(combined_text.split()[:5]) + \"...\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=combined_text,\n",
    "            metadata={\n",
    "                \"chunk_id\": idx,\n",
    "                \"source\": \"employee_handbook_india\",\n",
    "                \"length\": len(combined_text),\n",
    "                \"num_paragraphs\": len(chunk_group),\n",
    "                \"title\": title,\n",
    "                \"type\": \"policy\"  # Helps with filtering\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    if docs:\n",
    "        # Batch add documents\n",
    "        vectorstore.add_documents(docs)\n",
    "        return f\"Stored {len(docs)} semantic chunks in Chroma.\"\n",
    "    return \"No valid documents to store.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "384c640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial chunks...\n",
      "Created 72 initial chunks.\n",
      "Creating semantic chunks...\n",
      "Created 72 semantic chunks.\n",
      "Storing in Chroma...\n",
      "Stored 72 semantic chunks in Chroma.\n"
     ]
    }
   ],
   "source": [
    "# Process the document\n",
    "file_path = \"docs/policies.txt\"\n",
    "\n",
    "print(\"Creating initial chunks...\")\n",
    "paragraphs = create_initial_chunks(file_path)\n",
    "print(f\"Created {len(paragraphs)} initial chunks.\")\n",
    "\n",
    "print(\"Creating semantic chunks...\")\n",
    "semantic_chunks = create_semantic_chunks(paragraphs)\n",
    "print(f\"Created {len(semantic_chunks)} semantic chunks.\")\n",
    "\n",
    "print(\"Storing in Chroma...\")\n",
    "result = store_chunks_in_chroma(semantic_chunks)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6bb0508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    api_key=azure_openapi_api_key,\n",
    "    azure_endpoint=azure_openapi_azure_endpoint,\n",
    "    api_version=azure_openapi_api_version,\n",
    "    deployment_name=azure_openapi_deployment_name,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0df2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  \n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"fetch_k\": 10,   \n",
    "        \"lambda_mult\": 0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an HR policy expert. Generate 5 different versions of the user's question \n",
    "    to help retrieve relevant policy documents from a vector database. Focus on variations that \n",
    "    might appear in policy language. Provide these alternative versions separated by newlines.\n",
    "    \n",
    "    Original question: {question}\n",
    "    \n",
    "    Alternative versions:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    "    parser_key=\"lines\",  \n",
    "    include_original=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72868ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"You are a precise HR assistant at Ayatacommerce that answers questions using ONLY the provided context and consider you as an employee of ayatacommerce responsible for responding to all queries related to the company by other employees.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Always respond in a concise and professional manner and also just don't answer simply if always create a scentence with the answer.\n",
    "NOTE:Do not mention the source of the context or the document name in your answer or anything related to the provided context.\n",
    "If no information is avialable in the context, please respond with the following data:\n",
    "Contact HR at Ayatacommerce for assistance: hr@ayatacommerce.com \n",
    "Human Resourse email: hr@ayatacommerce.com\n",
    "\n",
    "Rules:\n",
    "1. If the context contains relevant information, provide a concise answer based solely on that.\n",
    "2. If the question asks about something NOT in the context, respond ONLY with: 'I don't have this information in my knowledge base. Please contact hr@ayatacommerce.com for assistance.'\n",
    "3. Never infer or make up information not explicitly stated in the context.\n",
    "4. If the question is ambiguous or unclear, ask for clarification.\n",
    "5. Do not provide any personal opinions or subjective statements.\n",
    "6. Always maintain a professional tone and language.\n",
    "7. Avoid using filler phrases like 'I think' or 'In my opinion'.\n",
    "8. If the context is too long, summarize it before answering.\n",
    "10. If the question is a yes/no question, provide a clear yes or no answer based on the context.\n",
    "11. If the question is a list, provide a clear and concise list based on the context.\n",
    "12. If the question is a how-to question, provide a clear and concise step-by-step guide based on the context.\n",
    "13. If the question is a why question, provide a clear and concise explanation based on the context.\n",
    "14. If the question is a when question, provide a clear and concise answer based on the context.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b77abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context(inputs):\n",
    "    docs = inputs[\"context\"]\n",
    "    print(\"ðŸ” Retrieved context:\\n\")\n",
    "    for doc in docs:\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 80)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b32a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e3d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_query(question: str) -> str:\n",
    "\n",
    "    start_time = time()\n",
    "    try:\n",
    "        response = query_chain.invoke(question)\n",
    "        elapsed = time() - start_time\n",
    "        print(f\"Response time: {elapsed:.2f} seconds\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8e790e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What is the remote work policy?\n",
      "Response time: 6.01 seconds\n",
      "Ayatacommerce has adopted a remote-first culture since 2018, emphasizing that individual value is not determined by hours spent at a desk. Employees are encouraged to create a designated workspace, ensure they have the necessary technology, and make remote work enjoyable by personalizing their environment. Regular check-ins with managers are recommended to maintain connection and support.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Query 1: What is the remote work policy?\")\n",
    "print(timed_query(\"What is the remote work policy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98381407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: How many sick leaves do employees get?\n",
      "Response time: 5.61 seconds\n",
      "Employees are entitled to 12 days of Casual/Sick Leaves each holiday year.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nQuery 2: How many sick leaves do employees get?\")\n",
    "print(timed_query(\"How many sick leaves do employees get?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74604eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: What are the core values of the company?\n",
      "Response time: 4.76 seconds\n",
      "The core values of the company are Empathy, Trust, and Adaptability, which guide our organizational identity and how we conduct our business.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nQuery 3: What are the core values of the company?\")\n",
    "print(timed_query(\"What are the core values of the company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a55f4305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 5.73 seconds\n",
      "The company is owned by Shine Mathew, who is the CEO and Founder of Ayatacommerce.\n"
     ]
    }
   ],
   "source": [
    "print(timed_query(\"Who is own the company\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ddb1986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 5.43 seconds\n",
      "I don't have this information in my knowledge base. Please contact hr@ayatacommerce.com for assistance.\n"
     ]
    }
   ],
   "source": [
    "print(timed_query(\"Is it possible to take a earned leave and a casual leave together?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timed_query(input(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78632dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
