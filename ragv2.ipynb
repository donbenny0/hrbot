{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d5f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db8de529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaf832",
   "metadata": {},
   "source": [
    "Ingesting TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "941cc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75bf9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a smaller, faster embedding model that still performs well\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # Smaller and faster than large version\n",
    "    model_kwargs={\"device\": \"cpu\"},  # Change to \"cuda\" if you have GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # Better for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f03caadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"docs/policies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30493a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_docs(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1886da",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6cce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_chunks():\n",
    "    text = read_docs(file_path)\n",
    "    # Split text by periods but keep the periods with the preceding text\n",
    "    paragraphs = [p + '.' for p in re.split(r'\\.', text)[:-1]]\n",
    "    # Add the last chunk without adding an extra period\n",
    "    if text and not text.endswith('.'):\n",
    "        paragraphs.append(re.split(r'\\.', text)[-1])\n",
    "    print(f\"Total paragraphs: {len(paragraphs)}\\n\")\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d71b6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_chunks():\n",
    "    text = read_docs(file_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    documents = text_splitter.create_documents([text])\n",
    "    extracted_texts = []\n",
    "    for doc in documents:\n",
    "        extracted_texts.append(doc.page_content)\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(create_initial_chunks()))\n",
    "print(create_initial_chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53902ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunks(paragraphs):\n",
    "    para_embeddings = [np.array(embedding_model.embed_query(paragraph)).reshape(1,-1) for paragraph in paragraphs]\n",
    "    semantic_chunks = []\n",
    "    for i in range(len(paragraphs)):\n",
    "        if i == 0:\n",
    "            semantic_chunks.append([paragraphs[i]])\n",
    "        else:\n",
    "            similarity = cosine_similarity(para_embeddings[i-1], para_embeddings[i])\n",
    "            if similarity[0][0] > 0.5:\n",
    "                semantic_chunks[-1].append(paragraphs[i])\n",
    "            else:\n",
    "                semantic_chunks.append([paragraphs[i]])\n",
    "\n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1e032",
   "metadata": {},
   "source": [
    "Vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7d2bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = create_initial_chunks()\n",
    "semantic_chunks = create_semantic_chunks(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1414483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"docs/chunk_rec_2.txt\"\n",
    "read_docs(file_path)\n",
    "semantic_chunks = read_docs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c991ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(semantic_chunks))\n",
    "print(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "becc91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"vectorstore_persist_3\"\n",
    "collection_name = \"vectorstore_table2_3\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c7a0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_in_chroma(semantic_chunks):\n",
    "    docs = []\n",
    "    for idx, chunk_group in enumerate(semantic_chunks):\n",
    "        combined_text = ' '.join(chunk_group).strip()\n",
    "        doc = Document(page_content=combined_text, metadata={\"chunk_id\": idx})\n",
    "        docs.append(doc)\n",
    "        print(docs)\n",
    "    vectorstore.add_documents(docs)\n",
    "    vectorstore.persist()\n",
    "    print(f\"Stored {len(docs)} semantic chunks in Chroma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_chunks_in_chroma(semantic_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d82ab",
   "metadata": {},
   "source": [
    "Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d4073648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core. runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7d383cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.1, max_tokens=512, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "77837ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate (\n",
    "input_variables=[\"question\"],\n",
    "template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from\n",
    "a vector database. By generating multiple perspectives on the user question, your\n",
    "goal is to help the user overcome some of the limitations of the distance-based\n",
    "similarity search. Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "217e084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm=llm, prompt=QUERY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "617ec53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a precise HR assistant that answers questions using ONLY the provided context.\n",
    "        \n",
    "        Always respond in a concise and professional manner and also just don't answer simply if always create a scentence with the answer.\n",
    "        Rules:\n",
    "        1. If the context contains relevant information, provide a concise answer based solely on that.\n",
    "        2. If the question asks about something NOT in the context, respond ONLY with: 'I don't have this information in my knowledge base. Please contact hr@ayatacommerce.com for assistance.'\n",
    "        3. Never infer or make up information not explicitly stated in the context.\n",
    "        4. If the question is ambiguous or unclear, ask for clarification.\n",
    "        5. Do not provide any personal opinions or subjective statements.\n",
    "        6. Always maintain a professional tone and language.\n",
    "        7. Avoid using filler phrases like 'I think' or 'In my opinion'.\n",
    "        8. If the context is too long, summarize it before answering.\n",
    "        9. If the context is too short, ask for more information.\n",
    "        10. If the question is a yes/no question, provide a clear yes or no answer based on the context.\n",
    "        11. If the question is a multiple-choice question, provide the best answer based on the context.\n",
    "        12. If the question is a definition, provide a clear and concise definition based on the context.\n",
    "        13. If the question is a comparison, provide a clear and concise comparison based on the context.\n",
    "        14. If the question is a list, provide a clear and concise list based on the context.\n",
    "        15. If the question is a how-to question, provide a clear and concise step-by-step guide based on the context.\n",
    "        16. If the question is a why question, provide a clear and concise explanation based on the context.\n",
    "        17. If the question is a when question, provide a clear and concise answer based on the context.\n",
    "        Context: {context}\n",
    "            \n",
    "        Question: {question}\n",
    "            \n",
    "        Answer:\"\"\"\n",
    "        \n",
    "prompt= ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "question = \"Who is the founder of the ayatacommerce?\"\n",
    "\n",
    "# Use the QUERY_PROMPT to format the prompt\n",
    "formatted_prompt = QUERY_PROMPT.format(question=question)\n",
    "\n",
    "# Call the LLM directly with the formatted prompt\n",
    "queries_output = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"Generated alternative queries:\\n\")\n",
    "print(queries_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd2507",
   "metadata": {},
   "source": [
    "Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a991d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=({\"context\":retriever, \"question\":RunnablePassthrough()} | prompt | llm | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.invoke({\"question\": \"What is the policy for remote work?\"})\n",
    "query.invoke({\"question\": \"Who is the founder of the ayatacommerce?\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
