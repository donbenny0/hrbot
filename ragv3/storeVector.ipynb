{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cdfde40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain-huggingface chromadb langchain-community langchain-core langchain-chroma sentence-transformers groq langchain_groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98332d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5e5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    API_VERSION = \"2024-02-01\"\n",
    "    PERSIST_DIRECTORY = \"askhr_bot_vectorstore\"\n",
    "    COLLECTION_NAME = \"askhr_bot_vectorstore_collection\"\n",
    "    CHUNK_SIZE = 500\n",
    "    CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1eb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    azure_endpoint=Config.AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=Config.AZURE_OPENAI_API_KEY,\n",
    "    openai_api_version=Config.API_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb908782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=Config.CHUNK_SIZE,\n",
    "    chunk_overlap=Config.CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[r\"\\n\\n\", r\"\\n\", r\"\\. \", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f50f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(file_path: str) -> str:\n",
    "    \"\"\"Read document content with error handling\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf10f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_chunks(file_path: str) -> List[str]:\n",
    "    \"\"\"Create initial text chunks\"\"\"\n",
    "    text = read_docs(file_path)\n",
    "    if not text:\n",
    "        return []\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    documents = text_splitter.create_documents([text])\n",
    "    return [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469ff442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure vector store with optimized settings\n",
    "persist_directory = \"askhr_bot_vectorstore\"\n",
    "collection_name = \"askhr_bot_vectorstore_collection\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2f1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunks(paragraphs: List[str], \n",
    "                         similarity_threshold: float = 0.82) -> List[List[str]]:\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "    \n",
    "    # Batch process all embeddings at once\n",
    "    para_embeddings = embedding_model.embed_documents(paragraphs)\n",
    "    para_embeddings = [np.array(e).reshape(1, -1) for e in para_embeddings]\n",
    "    \n",
    "    semantic_chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in range(len(paragraphs)):\n",
    "        if not current_chunk:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "            continue\n",
    "            \n",
    "        # Compare with all paragraphs in current chunk\n",
    "        similarities = [cosine_similarity(para_embeddings[i], e)[0][0] \n",
    "                       for e in para_embeddings[:i]]\n",
    "        max_similarity = max(similarities) if similarities else 0\n",
    "        \n",
    "        if max_similarity > similarity_threshold:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "        else:\n",
    "            semantic_chunks.append(current_chunk)\n",
    "            current_chunk = [paragraphs[i]]\n",
    "    \n",
    "    if current_chunk:\n",
    "        semantic_chunks.append(current_chunk)\n",
    "        \n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87f0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_in_chroma(semantic_chunks: List[List[str]]) -> str:\n",
    "    \"\"\"Store semantic chunks in Chroma with optimized metadata.\"\"\"\n",
    "    if not semantic_chunks:\n",
    "        return \"No chunks to store.\"\n",
    "    \n",
    "    docs = []\n",
    "    for idx, chunk_group in enumerate(semantic_chunks):\n",
    "        combined_text = ' '.join(chunk_group).strip()\n",
    "        if not combined_text:\n",
    "            continue\n",
    "            \n",
    "        first_sentence = combined_text.split('.')[0][:100]\n",
    "        word_count = len(combined_text.split())\n",
    "        para_count = len(chunk_group)\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=combined_text,\n",
    "            metadata={\n",
    "                \"chunk_id\": f\"chunk_{idx}_{word_count}words\",\n",
    "                \"source\": \"employee_handbook\",\n",
    "                \"length\": len(combined_text),\n",
    "                \"word_count\": word_count,\n",
    "                \"para_count\": para_count,\n",
    "                \"title\": first_sentence,\n",
    "                \"type\": \"policy\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    if docs:\n",
    "        # Batch add documents\n",
    "        vectorstore.add_documents(docs)\n",
    "        return f\"Stored {len(docs)} semantic chunks in Chroma.\"\n",
    "    return \"No valid documents to store.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c72cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path: str):\n",
    "    \"\"\"Complete document processing pipeline\"\"\"\n",
    "    print(\"Creating initial chunks...\")\n",
    "    paragraphs = create_initial_chunks(file_path)\n",
    "    print(f\"Created {len(paragraphs)} initial chunks.\")\n",
    "\n",
    "    print(\"Creating semantic chunks...\")\n",
    "    semantic_chunks = create_semantic_chunks(paragraphs)\n",
    "    print(f\"Created {len(semantic_chunks)} semantic chunks.\")\n",
    "\n",
    "    print(\"Storing in Chroma...\")\n",
    "    result = store_chunks_in_chroma(semantic_chunks)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8dd0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial chunks...\n",
      "Created 72 initial chunks.\n",
      "Creating semantic chunks...\n",
      "Created 72 semantic chunks.\n",
      "Storing in Chroma...\n",
      "Stored 72 semantic chunks in Chroma.\n"
     ]
    }
   ],
   "source": [
    "# Process the document\n",
    "file_path = \"../docs/policies.txt\"\n",
    "process_document(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
