{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "install-dependencies",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain-huggingface chromadb langchain-community langchain-core sentence-transformers groq langchain_groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing import List\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fee181be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "embedding-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DonBenny\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "text-splitter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[r\"\\n\\n\", r\"\\n\", r\"\\. \", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "file-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_chunks(file_path: str) -> List[str]:\n",
    "    \n",
    "    text = read_docs(file_path)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    documents = text_splitter.create_documents([text])\n",
    "    return [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "semantic-chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunks(paragraphs: List[str], \n",
    "                         similarity_threshold: float = 0.82) -> List[List[str]]:\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "    \n",
    "    # Batch process all embeddings at once\n",
    "    para_embeddings = embedding_model.embed_documents(paragraphs)\n",
    "    para_embeddings = [np.array(e).reshape(1, -1) for e in para_embeddings]\n",
    "    \n",
    "    semantic_chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in range(len(paragraphs)):\n",
    "        if not current_chunk:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "            continue\n",
    "            \n",
    "        # Compare with all paragraphs in current chunk\n",
    "        similarities = [cosine_similarity(para_embeddings[i], e)[0][0] \n",
    "                       for e in para_embeddings[:i]]\n",
    "        max_similarity = max(similarities) if similarities else 0\n",
    "        \n",
    "        if max_similarity > similarity_threshold:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "        else:\n",
    "            semantic_chunks.append(current_chunk)\n",
    "            current_chunk = [paragraphs[i]]\n",
    "    \n",
    "    if current_chunk:\n",
    "        semantic_chunks.append(current_chunk)\n",
    "        \n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vector-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DonBenny\\AppData\\Local\\Temp\\ipykernel_16040\\464282949.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Configure vector store with optimized settings\n",
    "persist_directory = \"vectorstore_persist_optimized_v1\"\n",
    "collection_name = \"vectorstore_table_optimized_v1\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "store-chunks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_in_chroma(semantic_chunks: List[List[str]]) -> str:\n",
    "    \"\"\"Store semantic chunks in Chroma with optimized metadata.\"\"\"\n",
    "    if not semantic_chunks:\n",
    "        return \"No chunks to store.\"\n",
    "    \n",
    "    docs = []\n",
    "    for idx, chunk_group in enumerate(semantic_chunks):\n",
    "        combined_text = ' '.join(chunk_group).strip()\n",
    "        if not combined_text:\n",
    "            continue\n",
    "            \n",
    "        # Extract first few words as title for better metadata\n",
    "        title = ' '.join(combined_text.split()[:5]) + \"...\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=combined_text,\n",
    "            metadata={\n",
    "                \"chunk_id\": idx,\n",
    "                \"source\": \"employee_handbook_india\",\n",
    "                \"length\": len(combined_text),\n",
    "                \"num_paragraphs\": len(chunk_group),\n",
    "                \"title\": title,\n",
    "                \"type\": \"policy\"  # Helps with filtering\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    if docs:\n",
    "        # Batch add documents\n",
    "        vectorstore.add_documents(docs)\n",
    "        return f\"Stored {len(docs)} semantic chunks in Chroma.\"\n",
    "    return \"No valid documents to store.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "process-document",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial chunks...\n",
      "Created 72 initial chunks.\n",
      "Creating semantic chunks...\n",
      "Created 57 semantic chunks.\n",
      "Storing in Chroma...\n",
      "Stored 57 semantic chunks in Chroma.\n"
     ]
    }
   ],
   "source": [
    "# Process the document\n",
    "file_path = \"docs/policies.txt\"\n",
    "\n",
    "print(\"Creating initial chunks...\")\n",
    "paragraphs = create_initial_chunks(file_path)\n",
    "print(f\"Created {len(paragraphs)} initial chunks.\")\n",
    "\n",
    "print(\"Creating semantic chunks...\")\n",
    "semantic_chunks = create_semantic_chunks(paragraphs)\n",
    "print(f\"Created {len(semantic_chunks)} semantic chunks.\")\n",
    "\n",
    "print(\"Storing in Chroma...\")\n",
    "result = store_chunks_in_chroma(semantic_chunks)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09f832e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  \n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"fetch_k\": 10,   \n",
    "        \"lambda_mult\": 0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an HR policy expert. Generate 5 different versions of the user's question \n",
    "    to help retrieve relevant policy documents from a vector database. Focus on variations that \n",
    "    might appear in policy language. Provide these alternative versions separated by newlines.\n",
    "    \n",
    "    Original question: {question}\n",
    "    \n",
    "    Alternative versions:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    "    parser_key=\"lines\",  \n",
    "    include_original=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "prompt-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"You are a precise HR assistant at Ayatacommerce that answers questions using ONLY the provided context and consider you as an employee of ayatacommerce responsible for responding to all queries related to the company by other employees.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Always respond in a concise and professional manner and also just don't answer simply if always create a scentence with the answer.\n",
    "NOTE:Do not mention the source of the context or the document name in your answer or anything related to the provided context.\n",
    "If no information is avialable in the context, please respond with the following data:\n",
    "Contact HR at Ayatacommerce for assistance: hr@ayatacommerce.com \n",
    "Human Resourse email: hr@ayatacommerce.com\n",
    "\n",
    "Rules:\n",
    "1. If the context contains relevant information, provide a concise answer based solely on that.\n",
    "2. If the question asks about something NOT in the context, respond ONLY with: 'I don't have this information in my knowledge base. Please contact hr@ayatacommerce.com for assistance.'\n",
    "3. Never infer or make up information not explicitly stated in the context.\n",
    "4. If the question is ambiguous or unclear, ask for clarification.\n",
    "5. Do not provide any personal opinions or subjective statements.\n",
    "6. Always maintain a professional tone and language.\n",
    "7. Avoid using filler phrases like 'I think' or 'In my opinion'.\n",
    "8. If the context is too long, summarize it before answering.\n",
    "10. If the question is a yes/no question, provide a clear yes or no answer based on the context.\n",
    "11. If the question is a list, provide a clear and concise list based on the context.\n",
    "12. If the question is a how-to question, provide a clear and concise step-by-step guide based on the context.\n",
    "13. If the question is a why question, provide a clear and concise explanation based on the context.\n",
    "14. If the question is a when question, provide a clear and concise answer based on the context.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "34ea7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context(inputs):\n",
    "    docs = inputs[\"context\"]\n",
    "    print(\"ðŸ” Retrieved context:\\n\")\n",
    "    for doc in docs:\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 80)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "test-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_query(question: str) -> str:\n",
    "\n",
    "    start_time = time()\n",
    "    try:\n",
    "        response = query_chain.invoke(question)\n",
    "        elapsed = time() - start_time\n",
    "        print(f\"Response time: {elapsed:.2f} seconds\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "test-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What is the remote work policy?\n",
      "Response time: 1.21 seconds\n",
      "Our company adopts a remote-first culture, allowing employees to work flexibly. To ensure productivity, we trust employees to determine what works best for them, but here are some tips: \n",
      "\n",
      "- Set up a designated workspace with necessary equipment.\n",
      "- Ensure a reliable and secure internet connection.\n",
      "- Get dressed for work to distinguish between work and personal life.\n",
      "- Write a daily to-do list with achievable tasks.\n",
      "- Know when to step away from work to avoid burnout.\n",
      "\n",
      "Additionally, we encourage employees to stay connected with colleagues through regular check-ins, video calls, and team chats. \n",
      "\n",
      "For more information on specific arrangements, employees can reach out to their manager or HR.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Query 1: What is the remote work policy?\")\n",
    "print(timed_query(\"What is the remote work policy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "test-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: How many sick leaves do employees get?\n",
      "Response time: 1.19 seconds\n",
      "Employees are entitled to 12 days of Casual/Sick Leaves each holiday year.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nQuery 2: How many sick leaves do employees get?\")\n",
    "print(timed_query(\"How many sick leaves do employees get?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "test-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: What are the core values of the company?\n",
      "Response time: 1.33 seconds\n",
      "The company's core values are Empathy, Trust, and Adaptability.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nQuery 3: What are the core values of the company?\")\n",
    "print(timed_query(\"What are the core values of the company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5eea3636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.02 seconds\n",
      "The ownership of the company is not explicitly stated in my knowledge base. \n",
      "\n",
      "Contact HR at Ayatacommerce for assistance: hr@ayatacommerce.com \n",
      "Human Resourse email: hr@ayatacommerce.com\n"
     ]
    }
   ],
   "source": [
    "print(timed_query(\"Who is own the company\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "188829b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.55 seconds\n",
      "According to our leave policy, you are entitled to 12 days of Earned Leaves and 12 days of Casual/Sick Leaves each holiday year. While the policy doesn't explicitly state that you can take both earned leave and casual leave together, it does allow you to carry over up to 10 days of earned leaves to the following year. However, to get a definitive answer on taking both leave types together, I would recommend checking with your manager or HR. \n",
      "\n",
      "However, a clear answer to your question is: yes, you can take both earned and casual leaves, but you have to check with your manager about the procedures.\n"
     ]
    }
   ],
   "source": [
    "print(timed_query(\"Is it possible to take a earned leave and a casual leave together?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8b178fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.08 seconds\n",
      "Taking short breaks throughout the day can help you feel recharged and refreshed, and give you a different perspective on any work problems. It is recommended to take short breaks, as well as at least half-an-hour to get some food.\n"
     ]
    }
   ],
   "source": [
    "print(timed_query(input(\"\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
