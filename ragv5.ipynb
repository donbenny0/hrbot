{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "install-dependencies",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain-huggingface chromadb langchain-community langchain-core sentence-transformers groq langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List, Dict, Any\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "embedding-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DonBenny\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "text-splitter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[r\"\\n\\n\", r\"\\n\", r\"\\. \", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "file-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_chunks(file_path: str) -> List[str]:\n",
    "    \n",
    "    text = read_docs(file_path)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    documents = text_splitter.create_documents([text])\n",
    "    return [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "semantic-chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_chunks(paragraphs: List[str], \n",
    "                         similarity_threshold: float = 0.82) -> List[List[str]]:\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "    \n",
    "    # Batch process all embeddings at once\n",
    "    para_embeddings = embedding_model.embed_documents(paragraphs)\n",
    "    para_embeddings = [np.array(e).reshape(1, -1) for e in para_embeddings]\n",
    "    \n",
    "    semantic_chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in range(len(paragraphs)):\n",
    "        if not current_chunk:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "            continue\n",
    "            \n",
    "        # Compare with all paragraphs in current chunk\n",
    "        similarities = [cosine_similarity(para_embeddings[i], e)[0][0] \n",
    "                       for e in para_embeddings[:i]]\n",
    "        \n",
    "        # Use max similarity rather than average for better grouping\n",
    "        max_similarity = max(similarities) if similarities else 0\n",
    "        \n",
    "        if max_similarity > similarity_threshold:\n",
    "            current_chunk.append(paragraphs[i])\n",
    "        else:\n",
    "            semantic_chunks.append(current_chunk)\n",
    "            current_chunk = [paragraphs[i]]\n",
    "    \n",
    "    if current_chunk:\n",
    "        semantic_chunks.append(current_chunk)\n",
    "        \n",
    "    return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vector-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DonBenny\\AppData\\Local\\Temp\\ipykernel_16040\\464282949.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Configure vector store with optimized settings\n",
    "persist_directory = \"vectorstore_persist_optimized_v1\"\n",
    "collection_name = \"vectorstore_table_optimized_v1\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}  # Optimize for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "store-chunks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_in_chroma(semantic_chunks: List[List[str]]) -> str:\n",
    "    \"\"\"Store semantic chunks in Chroma with optimized metadata.\"\"\"\n",
    "    if not semantic_chunks:\n",
    "        return \"No chunks to store.\"\n",
    "    \n",
    "    docs = []\n",
    "    for idx, chunk_group in enumerate(semantic_chunks):\n",
    "        combined_text = ' '.join(chunk_group).strip()\n",
    "        if not combined_text:\n",
    "            continue\n",
    "            \n",
    "        # Extract first few words as title for better metadata\n",
    "        title = ' '.join(combined_text.split()[:5]) + \"...\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=combined_text,\n",
    "            metadata={\n",
    "                \"chunk_id\": idx,\n",
    "                \"source\": \"employee_handbook_india\",\n",
    "                \"length\": len(combined_text),\n",
    "                \"num_paragraphs\": len(chunk_group),\n",
    "                \"title\": title,\n",
    "                \"type\": \"policy\"  # Helps with filtering\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    if docs:\n",
    "        # Batch add documents\n",
    "        vectorstore.add_documents(docs)\n",
    "        return f\"Stored {len(docs)} semantic chunks in Chroma.\"\n",
    "    return \"No valid documents to store.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "process-document",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating initial chunks...\n",
      "Created 72 initial chunks.\n",
      "Creating semantic chunks...\n",
      "Created 57 semantic chunks.\n",
      "Storing in Chroma...\n",
      "Stored 57 semantic chunks in Chroma.\n"
     ]
    }
   ],
   "source": [
    "# Process the document\n",
    "file_path = \"docs/policies.txt\"\n",
    "\n",
    "print(\"Creating initial chunks...\")\n",
    "paragraphs = create_initial_chunks(file_path)\n",
    "print(f\"Created {len(paragraphs)} initial chunks.\")\n",
    "\n",
    "print(\"Creating semantic chunks...\")\n",
    "semantic_chunks = create_semantic_chunks(paragraphs)\n",
    "print(f\"Created {len(semantic_chunks)} semantic chunks.\")\n",
    "\n",
    "print(\"Storing in Chroma...\")\n",
    "result = store_chunks_in_chroma(semantic_chunks)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-optimization",
   "metadata": {},
   "source": [
    "## Optimized Retrieval Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09f832e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the userdata dictionary with your API key\n",
    "userdata = {\n",
    "    \"GROQ_API_KEY\": \"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\"\n",
    "}\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    api_key=userdata.get(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\", \n",
    "    temperature=0.3, \n",
    "    top_k=10,\n",
    "    top_p=0.9,\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "retriever-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  \n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"fetch_k\": 10,   \n",
    "        \"lambda_mult\": 0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an HR policy expert. Generate 5 different versions of the user's question \n",
    "    to help retrieve relevant policy documents from a vector database. Focus on variations that \n",
    "    might appear in policy language. Provide these alternative versions separated by newlines.\n",
    "    \n",
    "    Original question: {question}\n",
    "    \n",
    "    Alternative versions:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    "    parser_key=\"lines\",  \n",
    "    include_original=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "prompt-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"You are a precise HR assistant at Ayatacommerce that answers questions using ONLY the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Rules:\n",
    "1. Respond concisely and professionally in 1-3 sentences.\n",
    "2. If the answer isn't in the context, say: \"I don't have this information in my knowledge base. Please contact hr@ayatacommerce.com for assistance.\"\n",
    "3. Never infer or make up information.\n",
    "4. For policy questions, cite the relevant policy section if possible.\n",
    "5. Maintain a professional tone.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34ea7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context(inputs):\n",
    "    docs = inputs[\"context\"]\n",
    "    print(\"🔍 Retrieved context:\\n\")\n",
    "    for doc in docs:\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 80)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "query-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "query_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing",
   "metadata": {},
   "source": [
    "## Testing the Optimized System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "test-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_query(question: str) -> str:\n",
    "\n",
    "    start_time = time()\n",
    "    try:\n",
    "        response = query_chain.invoke(question)\n",
    "        elapsed = time() - start_time\n",
    "        print(f\"Response time: {elapsed:.2f} seconds\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "test-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What is the remote work policy?\n",
      "Response time: 1.14 seconds\n",
      "AyataCommerce has adopted a remote-first culture, allowing employees to work flexibly. To ensure productivity, employees are encouraged to set up a designated workspace, have necessary tech, dress for work, and create a daily to-do list. For more information, refer to the relevant sections on remote work in the employee handbook.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Query 1: What is the remote work policy?\")\n",
    "print(timed_query(\"What is the remote work policy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "test-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 2: How many sick leaves do employees get?\n",
      "Response time: 1.26 seconds\n",
      "According to the provided context, employees are entitled to 12 days of Casual/Sick Leaves each holiday year, as mentioned in the \"HOLIDAYS\" section of the employee handbook. \n",
      "\n",
      "Reference: Document(metadata={'chunk_id':30, 'source': 'employee_handbook_india', 'title': 'If you are unable to...', 'length':1973, 'num_paragraphs':4, 'type': 'policy'}, page_content='If you are unable to work because of sickness or an emergency, you must ensure your manager is informed as early as possible on the first day of absence. HOLIDAYS If you work full-time on the India payroll you are entitled to12 days of Earned Leaves each holiday year,12 days of Casual/Sick Leaves.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nQuery 2: How many sick leaves do employees get?\")\n",
    "print(timed_query(\"How many sick leaves do employees get?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "test-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: What are the core values of the company?\n",
      "Response time: 1.05 seconds\n",
      "The core values of Ayatacommerce are Empathy, Trust, and Adaptability. These values govern our working practices, personal standards, and philosophies whenever we deal with Clients or each other. They are the essence of our organisational identity, acting as a guide for who we are and how we do things.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nQuery 3: What are the core values of the company?\")\n",
    "print(timed_query(\"What are the core values of the company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5eea3636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.48 seconds\n",
      "The CEO and Founder of AyataCommerce is Shine Mathew. This information can be found in the document titled \"New website launched2019...\" with chunk_id 4.\n"
     ]
    }
   ],
   "source": [
    "print(timed_query(\"Who is the ceo of Ayatacommerce?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
